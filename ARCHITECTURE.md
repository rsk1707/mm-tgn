# MM-TGN: Multimodal Temporal Graph Network
## Complete Architecture & API Documentation

**Version**: 2.3 (SLURM Jobs & MM-Fusion Ablation)  
**Last Updated**: December 4, 2025  
**Research Goal**: Solve Cold Start and Concept Drift in Recommendation Systems  
**Environment**: Great Lakes HPC (University of Michigan)

> ğŸ“– **See also**: [README.md](README.md) for quick start guide and high-level overview.
> This document contains detailed technical specifications for developers and researchers.

---

## Table of Contents
1. [Evaluation Protocol](#-evaluation-protocol-for-baseline-alignment)
2. [Research Hypothesis](#-research-hypothesis)
3. [System Architecture](#ï¸-system-architecture-diagram)
4. [Indexing Standard](#-indexing-standard-critical)
5. [Complete CLI Reference](#-complete-cli-reference)
6. [SLURM Job Scripts](#-slurm-job-scripts)
7. [Ready-to-Run Ablation Commands](#-ready-to-run-ablation-commands)
8. [Data Format Specifications](#-data-format-specifications)
9. [Core API Documentation](#-core-api-documentation)
10. [TensorBoard Setup](#-tensorboard-setup-great-lakes-hpc)
11. [File Dictionary](#-file-dictionary)
12. [Known Issues & Solutions](#-known-issues--solutions)
13. [Context Restoration](#-context-restoration-checklist)

---

## ğŸ“Š Evaluation Protocol (FOR BASELINE ALIGNMENT)

> **âš ï¸ CRITICAL FOR TEAMMATES**: Use these exact settings for fair comparison with baselines.

### Data Split Configuration

```python
# Code location: dataset.py, lines 65-66, 202-224

SPLIT_TYPE = "CHRONOLOGICAL"  # NOT random!
TRAIN_RATIO = 0.70            # 70% oldest interactions
VAL_RATIO = 0.15              # 15% middle interactions  
TEST_RATIO = 0.15             # 15% newest interactions
```

**Why Chronological (not Random)?**
1. TGN's memory mechanism requires temporal order
2. Prevents future data leakage
3. Naturally creates cold-start scenarios for inductive evaluation
4. Standard in temporal graph learning literature

### Ranking Evaluation Strategy

```python
# Code location: train_mmtgn.py, lines 344-480

RANKING_STRATEGY = "NEGATIVE_SAMPLING"  # NOT full ranking
N_NEGATIVES = 100                        # Default: rank positive among 100 negatives
```

**Evaluation Process:**
1. For each test edge (user_i, item_j):
2. Sample 100 random negative items (excluding item_j)
3. Compute scores for positive and all negatives
4. Rank positive among 101 candidates
5. Compute Recall@K, NDCG@K based on rank

**NOT Full Ranking** (ranking against all ~22K items) - too slow for TGN.

### Metrics Computed

| Metric | Formula | Code Location |
|--------|---------|---------------|
| Recall@K | hits(rank â‰¤ K) / N | `utils/metrics.py:57-89` |
| NDCG@K | Î£(1/log2(rank+1)) / N | `utils/metrics.py:92-130` |
| MRR | Î£(1/rank) / N | `utils/metrics.py:133-160` |
| AUC | P(pos > neg) | `utils/metrics.py:182-215` |
| AP | Same as MRR for single positive | `utils/metrics.py:218-235` |

### Evaluation Splits

| Split | Description | Purpose |
|-------|-------------|---------|
| **Overall** | All test interactions | General performance |
| **Transductive** | Both nodes seen in training | Warm-start |
| **Inductive** | â‰¥1 new node | **Cold-start (key metric!)** |

### Results Storage

```
checkpoints/<run_name>/
â”œâ”€â”€ best_model.pt           # Best model checkpoint
â”œâ”€â”€ train.log               # Full training log
â”œâ”€â”€ results_partial.json    # Link prediction results (saved early)
â””â”€â”€ results.json            # All metrics (saved at end)
```

### Baseline Alignment Checklist

```python
# Ensure your baseline uses:
assert split_type == "chronological"
assert train_ratio == 0.70
assert val_ratio == 0.15
assert test_ratio == 0.15
assert n_negatives == 100
assert metrics == ["Recall@10", "Recall@20", "NDCG@10", "NDCG@20", "MRR"]
```

---

## ğŸ¯ Research Hypothesis

> **Multimodal features modulated by temporal context solve the Cold Start problem better than ID-only baselines.**

**Dual-Channel Architecture:**
- **Channel 1**: Temporal Sequential Dynamics (TGN) + SOTA Multimodal Features âœ…
- **Channel 2**: Spectral/Structural Signal (LightGCN) - *In Development*

---

## ğŸ—ï¸ System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DATA PIPELINE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   ratings.csv â”€â”€â”¬â”€â”€â–º tgn_formatter.py â”€â”€â–º ml_*.csv (Temporal Edges)    â”‚
â”‚                 â”‚                              â”‚                        â”‚
â”‚   enriched.csv â”€â”¤                              â–¼                        â”‚
â”‚        +        â”‚                       ml_*.npy (Node Features)        â”‚
â”‚   posters/ â”€â”€â”€â”€â”€â”˜                              â”‚                        â”‚
â”‚                 â”‚                              â”‚                        â”‚
â”‚                 â–¼                              â”‚                        â”‚
â”‚   generate_embeddings.py                       â”‚                        â”‚
â”‚      â”‚                                         â”‚                        â”‚
â”‚      â”œâ”€â–º Qwen2-1.5B (Text: 1536-dim)          â”‚                        â”‚
â”‚      â””â”€â–º SigLIP-SO400M (Image: 1152-dim)      â”‚                        â”‚
â”‚              â”‚                                 â”‚                        â”‚
â”‚              â–¼                                 â”‚                        â”‚
â”‚      Concatenate: 2688-dim SOTA Features â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MODEL ARCHITECTURE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                     HybridNodeFeatures                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚ nn.Embedding â”‚    â”‚     MultimodalProjector                â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   (Users)    â”‚    â”‚  SOTA (2688-dim) â”€â”€â–º TGN (172-dim)    â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  10,200 Ã— 172â”‚    â”‚  OR Random nn.Embedding (ablation)    â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚         â”‚                        â”‚                               â”‚   â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚   â”‚
â”‚  â”‚                     â–¼                                            â”‚   â”‚
â”‚  â”‚           [padding=0 | users | items]                            â”‚   â”‚
â”‚  â”‚              Full Node Feature Matrix                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                    â”‚
â”‚                                    â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         TGN Backbone                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚   Memory     â”‚   â”‚   Temporal     â”‚   â”‚   Graph Attention  â”‚ â”‚   â”‚
â”‚  â”‚  â”‚   Module     â”‚â—„â”€â”€â”‚   Neighbor     â”‚â—„â”€â”€â”‚   Embedding        â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  (GRU/LSTM)  â”‚   â”‚   Finder       â”‚   â”‚   (2 layers)       â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                    â”‚
â”‚                                    â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    FiLM Fusion Layer                              â”‚  â”‚
â”‚  â”‚   Temporal Embeddings â”€â”€â”€â–º FiLMConditioner â—„â”€â”€â”€ Structural       â”‚  â”‚
â”‚  â”‚         (172-dim)           (Î³âŠ™x + Î²)           (Channel 2)      â”‚  â”‚
â”‚  â”‚              * BYPASS MODE if Channel 2 unavailable *            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    â”‚                                    â”‚
â”‚                                    â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      Prediction Head                              â”‚  â”‚
â”‚  â”‚            MergeLayer(user_emb, item_emb) â”€â”€â–º Score              â”‚  â”‚
â”‚  â”‚            Loss: BCE (default) or BPR (ranking)                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Indexing Standard (CRITICAL)

### 1-Based Indexing with Padding at 0

```
Index 0:                    PADDING (zero vector, used for neighbor masking)
Indices [1, num_users]:     USER nodes
Indices [num_users+1, ...]: ITEM nodes
```

**Why 1-based?**
- TGN's `NeighborFinder` uses 0 to indicate "no neighbor" (padding)
- All node IDs in CSV files are 1-indexed
- Feature matrix row 0 is always zeros

**ML-Modern Example:**
```
num_users  = 10,200
num_items  = 21,969
total_nodes = 32,170 (including padding)

User "1"     â†’ Index 1
User "10200" â†’ Index 10,200
Item (first) â†’ Index 10,201
Item (last)  â†’ Index 32,169
```

---

## ğŸ“‹ Complete CLI Reference

### Data Arguments

| Argument | Type | Default | Required | Description |
|----------|------|---------|----------|-------------|
| `--data-dir` | `str` | - | âœ… Yes | Directory containing `ml_*.csv`, `ml_*.npy`, `node_map.json` |
| `--dataset` | `str` | - | âœ… Yes | Dataset name (e.g., `ml-modern`, `amazon-sports`) |

### Model Architecture Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--embedding-dim` | `int` | `172` | Embedding dimension for all components |
| `--n-layers` | `int` | `2` | Number of graph attention layers |
| `--n-heads` | `int` | `2` | Number of attention heads per layer |
| `--n-neighbors` | `int` | `15` | Number of temporal neighbors to sample |
| `--memory-dim` | `int` | `172` | TGN memory vector dimension |
| `--message-dim` | `int` | `100` | Message passing dimension |
| `--dropout` | `float` | `0.1` | Dropout rate |

### Training Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--batch-size` | `int` | `200` | Training batch size |
| `--epochs` | `int` | `50` | Number of training epochs |
| `--lr` | `float` | `1e-4` | Learning rate |
| `--weight-decay` | `float` | `1e-5` | L2 regularization weight |
| `--patience` | `int` | `5` | Early stopping patience (epochs) |
| `--loss` | `str` | `bce` | Loss function: `bce` (stable) or `bpr` (ranking) |

### Feature & Model Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--use-memory` | `flag` | `True` | Enable TGN memory module |
| `--no-memory` | `flag` | - | Disable TGN memory module |
| `--use-hybrid` | `flag` | `True` | Use HybridNodeFeatures (learnable users + projected items) |
| `--no-hybrid` | `flag` | - | Use raw features directly |
| `--embedding-module` | `str` | `graph_attention` | Options: `graph_attention`, `graph_sum`, `identity`, `time` |

### Ablation Study Arguments

| Argument | Type | Default | Choices | Description |
|----------|------|---------|---------|-------------|
| `--node-feature-type` | `str` | `sota` | `sota`, `baseline`, `random` | **Experiment A**: Feature source |
| `--input-feat-dim` | `str` | `auto` | `auto`, `<int>` | **Experiment B**: Override feature dimension |
| `--fusion-mode` | `str` | `film` | `film`, `concat`, `none` | **Experiment C**: TGN+Channel2 fusion (bypass if no Ch2) |
| `--mm-fusion` | `str` | `mlp` | `mlp`, `film`, `gated` | **Experiment D**: Multimodal fusion (text + image) |

**Ablation Details:**

**Feature Source (`--node-feature-type`):**
- `sota`: Qwen2-1.5B + SigLIP-SO400M (2688-dim) - **Our method**
- `baseline`: MiniLM + CLIP (1536-dim) - **Comparison baseline**
- `random`: Learnable `nn.Embedding` (172-dim) - **Lower bound, no content**

**Multimodal Fusion (`--mm-fusion`):**
- `mlp`: Simple 2-layer MLP projection (default)
- `film`: FiLM conditioning - text modulates image: `Î³(text) âŠ™ img + Î²(text)`
- `gated`: Gated fusion - learned attention: `g âŠ™ text + (1-g) âŠ™ image`

### Evaluation Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--eval-ranking` | `flag` | `True` | Compute Recall@K, NDCG@K, MRR |
| `--no-eval-ranking` | `flag` | - | Skip ranking metrics (faster) |
| `--n-neg-eval` | `int` | `100` | Number of negatives per positive for ranking |
| `--eval-sample-size` | `int` | `None` | Subsample test set for faster ranking eval (e.g., `10000`). Use `None` for full evaluation (paper results). |

**Evaluation Speed Guide:**
| Mode | `--eval-sample-size` | `--n-neg-eval` | Est. Time |
|------|---------------------|----------------|-----------|
| **Development** | `10000` | `20` | ~3-5 min |
| **Validation** | `30000` | `50` | ~30 min |
| **Paper Results** | `None` (full) | `100` | ~6-8 hours |

### Logging Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--log-dir` | `str` | `runs` | TensorBoard log directory |
| `--save-dir` | `str` | `checkpoints` | Checkpoint save directory |
| `--run-name` | `str` | `None` | Custom run name (auto-generated: `mmtgn_<dataset>_<timestamp>`) |

### Other Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--seed` | `int` | `42` | Random seed for reproducibility |
| `--device` | `str` | `cuda` | Device: `cuda` or `cpu` |
| `--num-workers` | `int` | `0` | DataLoader workers (keep 0 for TGN) |

---

## ğŸ“¦ SLURM Job Scripts

Pre-configured job scripts are available in `jobs/` for easy submission:

### Quick Reference

| Script | Purpose | Time |
|--------|---------|------|
| `smoke_test.sh` | Quick verification (~5K samples) | 30 min |
| `train_ml_vanilla.sh` | Vanilla baseline (random features) | 6 hours |
| `train_ml_sota.sh` | SOTA + MLP fusion | 8 hours |
| `train_ml_sota_film.sh` | SOTA + FiLM fusion | 8 hours |
| `train_ml_sota_gated.sh` | SOTA + Gated fusion | 8 hours |
| `submit_all_ml.sh` | Submit ALL ablation experiments | - |

### Usage

```bash
# Single experiment
sbatch jobs/train_ml_sota.sh

# All ablations at once (4 jobs)
./jobs/submit_all_ml.sh

# Monitor
squeue -u $USER
tail -f logs/train_ml_*.out
```

### Job Output

```
logs/
â”œâ”€â”€ train_ml_vanilla_<jobid>.out    # Vanilla training output
â”œâ”€â”€ train_ml_sota_<jobid>.out       # SOTA+MLP output
â”œâ”€â”€ train_ml_sota_film_<jobid>.out  # SOTA+FiLM output
â””â”€â”€ train_ml_sota_gated_<jobid>.out # SOTA+Gated output

checkpoints/
â”œâ”€â”€ ml_vanilla_<date>/results.json  # Vanilla results
â”œâ”€â”€ ml_sota_mlp_<date>/results.json # SOTA+MLP results
â”œâ”€â”€ ml_sota_film_<date>/results.json
â””â”€â”€ ml_sota_gated_<date>/results.json
```

---

## ğŸš€ Ready-to-Run Ablation Commands

### Step 0: Environment Setup (Required First)

```bash
# Request GPU allocation on Great Lakes
salloc --account cse576f25s001_class \
       --partition gpu_mig40,gpu,spgpu \
       --nodes 1 --ntasks 1 --cpus-per-task 1 \
       --gpus 1 --mem 64G --time 06:00:00

# Activate environment
conda activate mmtgn
cd /scratch/cse576f25s001_class_root/cse576f25s001_class/huseynli/mm-tgn
```

### Experiment 0a: Quick Smoke Test (FAST - 5 min)

```bash
python train_mmtgn.py \
    --data-dir data/processed \
    --dataset ml-modern \
    --epochs 1 \
    --eval-ranking \
    --n-neg-eval 20 \
    --eval-sample-size 10000 \
    --run-name "smoke_test_fast"
```
**Expected**: ~5 min, verifies code runs without errors

### Experiment 0b: Full Smoke Test (1 Epoch - Full Evaluation)

```bash
python train_mmtgn.py \
    --data-dir data/processed \
    --dataset ml-modern \
    --embedding-dim 172 \
    --n-layers 2 \
    --n-neighbors 15 \
    --batch-size 200 \
    --epochs 1 \
    --lr 1e-4 \
    --loss bce \
    --eval-ranking \
    --n-neg-eval 100 \
    --run-name "smoke_test_full"
```
**Expected**: ~25 min, Loss ~0.5-0.6, AP ~0.75-0.80

---

### Experiment A: Vanilla Baseline (Random Features - LOWER BOUND)

**Quick Test (verify vanilla learns after bug fix):**
```bash
python train_mmtgn.py \
    --data-dir data/processed \
    --dataset ml-modern \
    --node-feature-type random \
    --fusion-mode none \
    --epochs 5 \
    --eval-ranking \
    --n-neg-eval 20 \
    --eval-sample-size 10000 \
    --run-name "vanilla_quick_test"
```
**Expected after fix**: Val AP should INCREASE (not stay at ~0.47). Target: AP > 0.55 by epoch 5.

**Full Experiment:**
```bash
python train_mmtgn.py \
    --data-dir data/processed \
    --dataset ml-modern \
    --node-feature-type random \
    --fusion-mode none \
    --embedding-dim 172 \
    --n-layers 2 \
    --n-neighbors 15 \
    --batch-size 200 \
    --epochs 50 \
    --lr 1e-4 \
    --loss bce \
    --patience 5 \
    --eval-ranking \
    --n-neg-eval 100 \
    --run-name "ablation_A_vanilla_random"
```
**Purpose**: Establishes lower bound. No semantic content - pure collaborative filtering.

---

### Experiment B1: SOTA Features (Our Full Method)

**Quick Test (verify ranking eval works after memory fix):**
```bash
python train_mmtgn.py \
    --data-dir data/processed \
    --dataset ml-modern \
    --node-feature-type sota \
    --epochs 1 \
    --eval-ranking \
    --n-neg-eval 20 \
    --eval-sample-size 10000 \
    --run-name "sota_ranking_test"
```
**Expected after fix**: Ranking evaluation should complete without "memory to time in past" error.

**Full Experiment:**
```bash
python train_mmtgn.py \
    --data-dir data/processed \
    --dataset ml-modern \
    --node-feature-type sota \
    --fusion-mode film \
    --embedding-dim 172 \
    --n-layers 2 \
    --n-neighbors 15 \
    --batch-size 200 \
    --epochs 50 \
    --lr 1e-4 \
    --loss bce \
    --patience 5 \
    --eval-ranking \
    --n-neg-eval 100 \
    --run-name "experiment_B1_sota_film"
```
**Purpose**: Full multimodal pipeline with Qwen2 + SigLIP features.

---

### Experiment B2: Baseline Encoders (Weaker Encoders)

First, generate baseline features (if not done):
```bash
python data/script/generate_embeddings.py \
    --csv-path data/datasets/movielens-32m/movielens-modern/ml-modern/enriched.csv \
    --image-dir data/datasets/movielens-32m/movielens-modern/ml-modern-posters \
    --output-dir data/datasets/movielens-32m/features/baseline \
    --dataset-name ml-modern \
    --id-col movieId \
    --text-col overview \
    --text-model baseline \
    --image-model clip \
    --batch-size 32
```

Then format for TGN:
```bash
python data/script/tgn_formatter.py \
    --ratings-path data/datasets/movielens-32m/movielens-modern/ml-modern/ratings.csv \
    --features-dir data/datasets/movielens-32m/features/baseline \
    --output-dir data/processed_baseline \
    --dataset-name ml-modern \
    --text-model baseline \
    --image-model clip
```

Train:
```bash
python train_mmtgn.py \
    --data-dir data/processed_baseline \
    --dataset ml-modern \
    --node-feature-type sota \
    --fusion-mode film \
    --embedding-dim 172 \
    --n-layers 2 \
    --n-neighbors 15 \
    --batch-size 200 \
    --epochs 50 \
    --lr 1e-4 \
    --loss bce \
    --patience 5 \
    --eval-ranking \
    --n-neg-eval 100 \
    --run-name "experiment_B2_baseline_encoders"
```
**Purpose**: Compare SOTA vs baseline encoder quality.

---

### Experiment C1: Fusion Mode - FiLM (Default)

```bash
python train_mmtgn.py \
    --data-dir data/processed \
    --dataset ml-modern \
    --node-feature-type sota \
    --fusion-mode film \
    --embedding-dim 172 \
    --epochs 50 \
    --eval-ranking \
    --run-name "experiment_C1_fusion_film"
```

### Experiment C2: Fusion Mode - Concatenation

```bash
python train_mmtgn.py \
    --data-dir data/processed \
    --dataset ml-modern \
    --node-feature-type sota \
    --fusion-mode concat \
    --embedding-dim 172 \
    --epochs 50 \
    --eval-ranking \
    --run-name "experiment_C2_fusion_concat"
```

### Experiment C3: Fusion Mode - None (Temporal Only)

```bash
python train_mmtgn.py \
    --data-dir data/processed \
    --dataset ml-modern \
    --node-feature-type sota \
    --fusion-mode none \
    --embedding-dim 172 \
    --epochs 50 \
    --eval-ranking \
    --run-name "experiment_C3_fusion_none"
```

---

### Experiment D: Loss Function Comparison

**D1: BCE Loss (Default, Stable)**
```bash
python train_mmtgn.py \
    --data-dir data/processed \
    --dataset ml-modern \
    --loss bce \
    --epochs 50 \
    --eval-ranking \
    --run-name "experiment_D1_loss_bce"
```

**D2: BPR Loss (Ranking-Optimized)**
```bash
python train_mmtgn.py \
    --data-dir data/processed \
    --dataset ml-modern \
    --loss bpr \
    --epochs 50 \
    --eval-ranking \
    --run-name "experiment_D2_loss_bpr"
```

---

## ğŸ“¦ Data Format Specifications

### Edge List CSV (`ml_<dataset>.csv`)

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `u` | `int` | Source node ID (1-based) | `1` |
| `i` | `int` | Destination node ID (1-based) | `10205` |
| `ts` | `float` | Unix timestamp | `1609459200.0` |
| `label` | `int` | Edge label (always 1 for ratings) | `1` |
| `idx` | `int` | Edge index (1-based) | `1` |

**Example:**
```csv
u,i,ts,label,idx
1,10205,1609459200.0,1,1
1,10892,1609459500.0,1,2
2,10205,1609460000.0,1,3
```

### Node Features NPY (`ml_<dataset>.npy`)

- **Shape**: `(n_nodes, feature_dim)` = `(32170, 2688)` for ML-Modern SOTA
- **Row 0**: All zeros (padding)
- **Rows 1-10200**: User embeddings (projected or zeros)
- **Rows 10201-32169**: Item embeddings (SOTA multimodal)

### Node Map JSON (`node_map.json`)

```json
{
  "num_users": 10200,
  "num_items": 21969,
  "feature_dim": 2688,
  "padding_idx": 0,
  "user_range": [1, 10200],
  "item_range": [10201, 32169]
}
```

---

## ğŸ”§ Core API Documentation

### `train_mmtgn.py`

**Main Function:**
```python
def train(args: argparse.Namespace) -> Dict[str, float]:
    """
    Main training loop for MM-TGN.
    
    Args:
        args: Parsed command-line arguments
        
    Returns:
        Dictionary of final evaluation metrics
    """
```

### `mmtgn.py`

**MMTGN Class:**
```python
class MMTGN(nn.Module):
    """
    Multimodal Temporal Graph Network.
    
    Args:
        neighbor_finder: NeighborFinder - Temporal neighbor lookup
        node_features: np.ndarray - Shape (n_nodes, feat_dim)
        edge_features: np.ndarray - Shape (n_edges, edge_dim)
        device: str - 'cuda' or 'cpu'
        n_layers: int - Number of graph attention layers (default: 2)
        n_heads: int - Attention heads (default: 2)
        embedding_dim: int - Working dimension (default: 172)
        dropout: float - Dropout rate (default: 0.1)
        use_memory: bool - Enable TGN memory (default: True)
        memory_dimension: int - Memory vector size (default: 172)
        num_users: int - Number of users (for hybrid features)
        num_items: int - Number of items (for hybrid features)
        item_features: np.ndarray - SOTA item embeddings
        use_hybrid_features: bool - Use HybridNodeFeatures (default: True)
        use_random_item_features: bool - Ablation: random embeddings (default: False)
        use_film: bool - Enable FiLM fusion (default: True)
        structural_dim: int - Channel 2 dimension (None = bypass)
    """
    
    def compute_temporal_embeddings(
        self,
        source_nodes: np.ndarray,
        destination_nodes: np.ndarray,
        negative_nodes: np.ndarray,
        edge_times: np.ndarray,
        edge_idxs: np.ndarray,
        n_neighbors: int = 20,
        structural_embeddings: Optional[torch.Tensor] = None,
        skip_memory_update: bool = False  # NEW: For evaluation
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Compute temporal embeddings for a batch.
        
        Args:
            skip_memory_update: If True, skip memory update step. Use for
                               negative scoring in evaluation to avoid 
                               "update memory to time in past" errors.
        
        Returns:
            (source_emb, dest_emb, neg_emb) - All shape [batch, embedding_dim]
        """
    
    def compute_edge_probabilities(
        self,
        source_nodes: np.ndarray,
        destination_nodes: np.ndarray,
        negative_nodes: np.ndarray,
        edge_times: np.ndarray,
        edge_idxs: np.ndarray,
        n_neighbors: int = 20,
        skip_memory_update: bool = False  # NEW: For evaluation
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute link probabilities.
        
        Args:
            skip_memory_update: If True, skip memory update. Use for negative
                               scoring in evaluation to avoid timestamp conflicts.
        
        Returns:
            (pos_probs, neg_probs) - Both shape [batch]
        """
```

**Factory Function:**
```python
def create_mmtgn(
    dataset: TemporalDataset,
    device: str = "cuda",
    embedding_dim: int = 172,
    n_layers: int = 2,
    n_heads: int = 2,
    n_neighbors: int = 20,
    memory_dim: int = 172,
    message_dim: int = 100,
    dropout: float = 0.1,
    use_memory: bool = True,
    use_hybrid_features: bool = True,
    embedding_module_type: str = "graph_attention",
    structural_dim: Optional[int] = None,
    use_random_item_features: bool = False
) -> MMTGN:
    """Create MMTGN from TemporalDataset."""
```

**Loss Functions:**
```python
def bpr_loss(pos_scores: torch.Tensor, neg_scores: torch.Tensor) -> torch.Tensor:
    """Bayesian Personalized Ranking loss."""

def bce_loss(pos_scores: torch.Tensor, neg_scores: torch.Tensor, pos_weight: float = 1.0) -> torch.Tensor:
    """Binary Cross-Entropy loss."""
```

### `modules/embedding.py`

**HybridNodeFeatures:**
```python
class HybridNodeFeatures(nn.Module):
    """
    Unified node feature provider.
    
    Args:
        num_users: int - Number of user nodes
        num_items: int - Number of item nodes
        item_features: np.ndarray - SOTA features (num_items, feat_dim)
        embedding_dim: int - Target dimension (172)
        dropout: float - Projector dropout (0.1)
        freeze_items: bool - Freeze SOTA features (True)
        use_random_items: bool - Ablation mode (False)
    
    Methods:
        forward(node_ids: torch.Tensor) -> torch.Tensor:
            Get features for node IDs. Shape: (batch, embedding_dim)
        
        get_all_features() -> torch.Tensor:
            Full feature matrix. Shape: (total_nodes, embedding_dim)
    """
```

**FiLMConditioner:**
```python
class FiLMConditioner(nn.Module):
    """
    Feature-wise Linear Modulation.
    
    Formula: output = Î³(cond) âŠ™ input + Î²(cond)
    
    Args:
        input_dim: int - Input feature dimension
        cond_dim: int - Conditioning feature dimension
        hidden_dim: int - Hidden layer dimension (optional)
    """
```

**UserStateFiLM:**
```python
class UserStateFiLM(nn.Module):
    """
    User-State FiLM for cold-start adaptation.
    
    Uses user's TGN memory to modulate item features.
    
    Formula: h_adapted = Î³(h_user) âŠ™ h_item + Î²(h_user)
    
    Args:
        user_dim: int - User memory dimension (172)
        item_dim: int - Item feature dimension (172)
        hidden_dim: int - Hidden dimension (optional)
    """
```

### `utils/metrics.py`

**RankingMetrics:**
```python
@dataclass
class RankingMetrics:
    """Container for ranking evaluation results."""
    recall_at_10: float
    recall_at_20: float
    ndcg_at_10: float
    ndcg_at_20: float
    mrr: float
    hit_rate_at_10: float
    hit_rate_at_20: float
    auc: float
    ap: float
```

**Metric Functions:**
```python
def compute_recall_at_k(pos_scores: np.ndarray, neg_scores_matrix: np.ndarray, k: int) -> float:
    """Compute Recall@K."""

def compute_ndcg_at_k(pos_scores: np.ndarray, neg_scores_matrix: np.ndarray, k: int) -> float:
    """Compute NDCG@K."""

def compute_mrr(pos_scores: np.ndarray, neg_scores_matrix: np.ndarray) -> float:
    """Compute Mean Reciprocal Rank."""

def compute_all_ranking_metrics(
    model: MMTGN,
    data: Data,
    neg_sampler: NegativeSamplerForEval,
    n_neighbors: int,
    batch_size: int,
    device: str
) -> RankingMetrics:
    """Compute all ranking metrics for a dataset split."""
```

---

## ğŸ“Š TensorBoard Setup (Great Lakes HPC)

### Log Locations
```
/scratch/.../mm-tgn/
â”œâ”€â”€ runs/                          # TensorBoard event files
â”‚   â””â”€â”€ mmtgn_<dataset>_<timestamp>/
â”‚       â””â”€â”€ events.out.tfevents.*
â”œâ”€â”€ checkpoints/                   # Model checkpoints
â”‚   â””â”€â”€ mmtgn_<dataset>_<timestamp>/
â”‚       â”œâ”€â”€ best_model.pt
â”‚       â””â”€â”€ train.log
```

### Quick Start
```bash
# On compute node
cd /scratch/cse576f25s001_class_root/cse576f25s001_class/huseynli/mm-tgn
./scripts/launch_tensorboard.sh &

# Or manually (IPv4 only - required for HPC):
tensorboard --logdir=runs --port=6006 --host=0.0.0.0 &
```

### SSH Tunnel (from LOCAL machine)
```bash
# Replace gl1509 with your compute node
ssh -L 6006:gl1509.arc-ts.umich.edu:6006 huseynli@greatlakes.arc-ts.umich.edu
```

Open: **http://localhost:6006**

### Management Commands
| Action | Command |
|--------|---------|
| Start | `./scripts/launch_tensorboard.sh &` |
| Check | `ps aux \| grep tensorboard` |
| Stop | `pkill -f tensorboard` |
| Custom port | `TENSORBOARD_PORT=6007 ./scripts/launch_tensorboard.sh &` |

---

## ğŸ“ File Dictionary

### Core Model (`/mm-tgn/`)

| File | Lines | Responsibility |
|------|-------|---------------|
| `mmtgn.py` | ~700 | MMTGN model, FiLM fusion, loss functions, factory |
| `dataset.py` | ~500 | TemporalDataset, temporal splits, inductive tracking |
| `train_mmtgn.py` | ~900 | Training loop, evaluation, CLI arguments |

### Modules (`/mm-tgn/modules/`)

| File | Lines | Responsibility |
|------|-------|---------------|
| `embedding.py` | ~770 | HybridNodeFeatures, FiLM, UserStateFiLM, TGN embeddings |
| `memory.py` | ~100 | TGN Memory class |
| `memory_updater.py` | ~150 | GRU/RNN/LSTM memory updaters |
| `message_aggregator.py` | ~80 | Message aggregation (last, mean) |
| `message_function.py` | ~60 | Message computation (MLP, identity) |

### Data Pipeline (`/mm-tgn/data/script/`)

| File | Lines | Responsibility |
|------|-------|---------------|
| `generate_embeddings.py` | ~300 | Universal multimodal encoder |
| `tgn_formatter.py` | ~200 | ratings + features â†’ TGN format |
| `movielens_scraper.py` | ~400 | TMDB API scraper |
| `movielens_subset.py` | ~200 | ML-Modern subset creation |

### Utilities (`/mm-tgn/utils/`)

| File | Lines | Responsibility |
|------|-------|---------------|
| `utils.py` | ~400 | NeighborFinder, RandEdgeSampler, EarlyStopMonitor |
| `metrics.py` | ~400 | Recall@K, NDCG@K, MRR, evaluation functions |

---

## ğŸ”¢ Dimension Reference

| Component | Dimension | Notes |
|-----------|-----------|-------|
| TGN Embedding | 172 | From TGN paper |
| TGN Memory | 172 | Matches embedding |
| SOTA Text (Qwen2) | 1536 | |
| SOTA Image (SigLIP) | 1152 | SO400M |
| **SOTA Combined** | **2688** | |
| Baseline Text (MiniLM) | 768 | |
| Baseline Image (CLIP) | 768 | ViT-L-14 |
| **Baseline Combined** | **1536** | |
| User Embedding | 172 | Learnable |
| Item Projected | 172 | After projector |

---

## ğŸš¨ Known Issues & Solutions

| Issue | Solution | Status |
|-------|----------|--------|
| `torch.load` WeightsOnly error (PyTorch 2.6) | Use `weights_only=False` | âœ… Fixed |
| `ReduceLROnPlateau` verbose parameter | Remove `verbose=True` | âœ… Fixed |
| TensorBoard IPv6 binding error | Use `--host=0.0.0.0` | âœ… Fixed |
| Memory assertion "update to time in past" | Added `skip_memory_update=True` for negative scoring | âœ… Fixed |
| Vanilla model not learning (AP â‰ˆ 0.47) | Fixed gradient flow: use direct assignment instead of `.data.copy_()` | âœ… Fixed |
| Gradient warning (non-leaf tensor) | Harmless, ignore | âš ï¸ Cosmetic |
| PyG extension warnings | Fallback works | âš ï¸ Non-blocking |

### Critical Bug Details

**Bug 1: Memory Update Assertion Error**
```
AssertionError: Trying to update memory to time in the past
```
- **Cause**: Negative scoring in `evaluate_ranking` used repeated timestamps
- **Fix**: Added `skip_memory_update=True` parameter to skip memory updates when scoring negatives
- **Location**: `mmtgn.py` (`compute_edge_probabilities`, `compute_temporal_embeddings`)

**Bug 2: Vanilla Model Not Learning**
- **Symptom**: Val AP stayed at ~0.47 (below random 0.5) for all epochs
- **Cause**: `.data.copy_()` broke gradient flow to learnable item embeddings
- **Fix**: Changed to direct tensor assignment in `update_node_features()`
- **Location**: `mmtgn.py` line ~302

```python
# BEFORE (broken - no gradients flow):
self.node_raw_features.data.copy_(all_features)

# AFTER (fixed - gradients preserved):
self.embedding_module.node_features = all_features
```

---

## ğŸ“Š Expected Metrics

### Smoke Test (1 Epoch)
| Metric | SOTA Expected | Vanilla Expected | Notes |
|--------|---------------|------------------|-------|
| Train Loss | 0.5 - 0.7 | 0.6 - 0.7 | Should decrease |
| Val AP | 0.75 - 0.85 | 0.55 - 0.70 | Better than 0.5 |
| Val AUC | 0.75 - 0.85 | 0.55 - 0.70 | Better than 0.5 |
| Time | ~25 min | ~20 min | On 1 GPU |

### Full Training (50 Epochs)
| Metric | SOTA Transductive | SOTA Inductive | Vanilla | Notes |
|--------|-------------------|----------------|---------|-------|
| Test AP | > 0.85 | > 0.80 | > 0.60 | SOTA >> Vanilla = success |
| Test AUC | > 0.85 | > 0.80 | > 0.60 | |
| Recall@10 | > 0.05 | > 0.02 | > 0.01 | Higher = better |
| NDCG@10 | > 0.03 | > 0.01 | > 0.01 | Higher = better |

### Key Comparisons (Research Validation)
| Comparison | Expected Outcome | What It Proves |
|------------|------------------|----------------|
| SOTA AP > Vanilla AP | +0.15 to +0.25 | Multimodal features help |
| SOTA Inductive > Vanilla Inductive | Larger gap | Cold-start hypothesis |
| SOTA > Baseline | +0.05 to +0.10 | Better encoders matter |

---

## ğŸ”„ Context Restoration Checklist

```bash
# 1. Read this documentation
cat ARCHITECTURE.md

# 2. Request GPU
salloc --account cse576f25s001_class \
       --partition gpu_mig40,gpu,spgpu \
       --gpus 1 --mem 64G --time 06:00:00

# 3. Activate environment
conda activate mmtgn

# 4. Navigate to project
cd /scratch/cse576f25s001_class_root/cse576f25s001_class/huseynli/mm-tgn

# 5. Verify data
ls -la data/processed/
# Should see: ml_ml-modern.csv, ml_ml-modern.npy, node_map.json

# 6. Run smoke test
python train_mmtgn.py --data-dir data/processed --dataset ml-modern --epochs 1

# 7. Start TensorBoard
./scripts/launch_tensorboard.sh &

# 8. SSH tunnel (from LOCAL machine)
# ssh -L 6006:gl1509.arc-ts.umich.edu:6006 huseynli@greatlakes.arc-ts.umich.edu

# 9. View TensorBoard
# Open: http://localhost:6006
```

---

## ğŸ“ Changelog

### 2025-12-04 (v2.3) - SLURM Jobs & MM-Fusion Ablation
- **ğŸ“¦ NEW: SLURM Job Scripts** (`jobs/` folder)
  - `smoke_test.sh` - Quick verification
  - `train_ml_vanilla.sh` - Vanilla baseline
  - `train_ml_sota.sh` - SOTA + MLP fusion
  - `train_ml_sota_film.sh` - SOTA + FiLM fusion
  - `train_ml_sota_gated.sh` - SOTA + Gated fusion
  - `submit_all_ml.sh` - Submit all experiments at once
- **ğŸ”¬ NEW: `--mm-fusion` CLI argument**
  - `mlp`: Simple 2-layer MLP projection (default)
  - `film`: FiLM conditioning (text modulates image)
  - `gated`: Gated fusion (learned attention weights)
- **ğŸ“Š Confirmed 3-Group Evaluation** (train_mmtgn.py)
  - Overall: All test interactions
  - Transductive: Users seen in training (fair comparison with LOO)
  - Inductive: Cold-start users (MM-TGN advantage)
- **ğŸ“š Updated README.md & ARCHITECTURE.md**
  - Added jobs folder documentation
  - Added mm-fusion ablation commands
  - Updated CLI reference

### 2025-12-04 (v2.2) - Evaluation Protocol Documentation
- **ğŸ“Š NEW: Evaluation Protocol Section**
  - Documented exact split ratios (70/15/15 chronological)
  - Documented ranking strategy (100 negatives per positive)
  - Added baseline alignment checklist for teammates
- **ğŸ’¾ NEW: Early Results Saving**
  - Added `results_partial.json` saved before slow ranking evaluation
  - Prevents losing results if ranking eval is interrupted
- **ğŸ“š Updated README.md**
  - Made complementary to ARCHITECTURE.md (high-level vs technical)
  - Added evaluation protocol summary for teammates
  - Added project structure overview

### 2025-12-03 (v2.1) - Critical Bug Fixes
- **ğŸ› CRITICAL FIX: Memory Assertion Error**
  - Added `skip_memory_update` parameter to `compute_edge_probabilities()` and `compute_temporal_embeddings()`
  - Fixes `AssertionError: Trying to update memory to time in the past` during ranking evaluation
- **ğŸ› CRITICAL FIX: Vanilla Model Not Learning**
  - Fixed gradient flow for random item embeddings
  - Changed `.data.copy_()` to direct tensor assignment in `update_node_features()`
  - Vanilla model now learns properly (AP should improve from ~0.47)
- **âš¡ NEW: `--eval-sample-size` argument**
  - Subsample test set for faster ranking evaluation during development
  - Use `--eval-sample-size 10000 --n-neg-eval 20` for ~75x faster evaluation
- **ğŸ“š Updated API documentation** with new parameters

### 2025-12-03 (v2.0)
- **Complete CLI Reference**: All arguments with types, defaults, descriptions
- **Ready-to-Run Commands**: Copy-paste commands for all ablation experiments
- **Data Format Specs**: Detailed CSV/NPY/JSON format documentation
- **Core API Documentation**: Function signatures and descriptions
- **Added Table of Contents**

### 2025-12-03 (v1.1)
- Added TensorBoard setup for Great Lakes HPC
- Added `scripts/launch_tensorboard.sh` with IPv4 fix
- Fixed `verbose` parameter in ReduceLROnPlateau
- Fixed `torch.load` for PyTorch 2.6

### 2025-12-03 (v1.0)
- Added ablation CLI arguments
- Implemented `UserStateFiLM`
- Fixed Amazon category parsing
- Created initial documentation
